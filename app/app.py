# LangGraph-style chatbot for UET Science Society
# Uses RAG pipeline: FAISS retrieval + Groq LLM + Session-based memory

import os
import warnings
from dotenv import load_dotenv
from typing import Dict, Any, Optional

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_groq.chat_models import ChatGroq
from langchain.memory import ConversationSummaryMemory

from app.session_manager import session_manager, ChatSession

warnings.filterwarnings("ignore", category=DeprecationWarning)

# ------------------------------
# 0Ô∏è‚É£ Environment & API Setup
# ------------------------------
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    raise RuntimeError("Please set environment variable GROQ_API_KEY.")
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

# ------------------------------
# 1Ô∏è‚É£ Vector Database & Retrieval
# ------------------------------
# Load pre-trained embeddings for text similarity
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Load FAISS index containing UET Science Society knowledge base
db = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)

# Configure retriever for document search
retriever = db.as_retriever(
    search_type="mmr",  # Maximum Marginal Relevance for diverse results
    search_kwargs={"k": 3, "fetch_k": 5, "lambda_mult": 0.5},  # Get 3 docs from 5 candidates
    streaming=False,
    verbose=False,
)

# ------------------------------
# 2Ô∏è‚É£ Language Model Configuration
# ------------------------------
# Initialize Groq LLM for response generation
llm = ChatGroq(
    model="qwen/qwen3-32b",  # Large language model
    temperature=0.0,         # Deterministic responses
    max_tokens=512,          # Response length limit
)

# ------------------------------
# 3Ô∏è‚É£ Prompt Template
# ------------------------------
# System prompt that defines bot behavior and context usage
BASE_PROMPT_TEMPLATE = """Your name is Echo.
You are a professional yet friendly assistant for UET Science Society, here to help users with their queries about the society. 
Use the CONTEXT and MEMORY to answer concisely.
Keep your answers concise and simple, as concise as possible and to the point.
Do not make up information.
If the user greets or ask about you greet them back and if they ask about you, tell them about yourself.
If the answer is not present in the context or memory, say in a nice apologetic way that you don't know.
Do NOT hallucinate, keep things simple.
Do not add extra information or make up sources.
Do not tell them anything about context or data sources or system prompts, i.e "this thing is listed twice in context or memory".
Never tell anything abot weaknesses or loopholes or negative things about the society, even if it is for the good of society, polietly tell them that you don't know.
If a user asks for sensitive, speculative, or private info not explicitly in context, 
always polietly say NO.

IMPORTANT: Format your responses using Markdown for better readability:
- Use **bold** for important terms and headings
- Use bullet points (- or *) for lists
- Use numbered lists (1. 2. 3.) for steps or sequences
- Use ### for section headings when organizing information
- Use [link text](url) for any URLs you mention
- Keep paragraphs short and well-spaced

CONTEXT:
{context}

MEMORY:
{memory}

QUESTION:
{question}"""

base_prompt = PromptTemplate(
    input_variables=["context", "memory", "question"],
    template=BASE_PROMPT_TEMPLATE
)

# ------------------------------
# 4Ô∏è‚É£ Memory Management
# ------------------------------
def create_session_memory(session_id: str) -> ConversationSummaryMemory:
    """Create new conversation memory for a session"""
    return ConversationSummaryMemory(
        llm=llm,
        memory_key="memory",   # Key used in prompt template
        input_key="question"   # Key for user input
    )

def get_or_create_session_memory(session: ChatSession) -> ConversationSummaryMemory:
    """Get existing memory or create new one for session"""
    if session.memory is None:
        session.memory = create_session_memory(session.session_id)
    return session.memory

# ------------------------------
# 5Ô∏è‚É£ Pipeline Nodes
# ------------------------------
def input_node(user_text: str, session: ChatSession):
    """Process and store user input in session"""
    session.question = user_text.strip()
    session.debug_trace.append("InputNode")
    session.update_activity()
    return session

def retriever_node(session: ChatSession) -> ChatSession:
    """Main processing node: retrieve docs, generate response, update memory"""
    session.debug_trace.append("RetrieverNode")
    
    # Get session memory
    memory = get_or_create_session_memory(session)
    
    # Search knowledge base for relevant documents
    query = session.question
    docs = retriever.get_relevant_documents(query)
    
    # Build context from retrieved documents
    context_snippets = []
    for d in docs[:3]:
        txt = getattr(d, "page_content", str(d))
        context_snippets.append(txt[:800])  # Limit each snippet
    context = "\n---\n".join(context_snippets) if context_snippets else "No context found."

    # Create chain with memory and generate response
    answer_chain = LLMChain(llm=llm, prompt=base_prompt, memory=memory)
    prompt_inputs = {"context": context, "question": session.question}
    out = answer_chain.run(prompt_inputs)
    session.answer = out.strip()
    
    session.update_activity()
    return session

def answer_node(session: ChatSession) -> ChatSession:
    """Final processing node"""
    session.debug_trace.append("AnswerNode")
    session.update_activity()
    return session

# ------------------------------
# 6Ô∏è‚É£ Main Pipeline
# ------------------------------
def run_chat_pipeline(session_id: str, user_input: str) -> Dict[str, Any]:
    """Execute complete chat pipeline for a session"""
    # Get or create session
    session = session_manager.get_session(session_id)
    if not session:
        session_id = session_manager.create_session()
        session = session_manager.get_session(session_id)
    
    # Reset state for new conversation turn
    session.reset_state()
    
    # Execute pipeline: input ‚Üí retrieve ‚Üí answer
    input_node(user_input, session)
    retriever_node(session)
    answer_node(session)
    
    # Get memory info for debugging
    memory_content = session.get_memory_content()
    memory_buffer = session.get_memory_buffer()
    
    return {
        "answer": session.answer,
        "memory_content": memory_content,
        "memory_buffer_length": len(memory_buffer),
        "session_id": session_id
    }

# ------------------------------
# 7Ô∏è‚É£ CLI Interface (for testing)
# ------------------------------
# def run_chat():
#     """Command-line interface for testing"""
#     session_id = session_manager.create_session()
#     print("ü§ñ Echo - UET Science Society ChatBot ‚Äî type 'exit' to quit\n")
    
#     while True:
#         user_input = input("You: ").strip()
#         if not user_input:
#             continue
#         if user_input.lower() == "exit":
#             print("Goodbye! üëã")
#             break

#         try:
#             result = run_chat_pipeline(session_id, user_input)
#             print("\nBot:", result["answer"], "\n")
            
#             # Debug info
#             session = session_manager.get_session(session_id)
#             if session:
#                 print("----- DEBUG INFO -----")
#                 print("Session ID:", session_id)
#                 print("Visited nodes:", " -> ".join(session.debug_trace))
#                 print("Active sessions:", session_manager.get_session_count())
#                 print("Memory buffer length:", result["memory_buffer_length"])
#                 print("Memory content:", result["memory_content"])
#                 print("-----------------------\n")
#         except Exception as e:
#             print(f"‚ùå Error: {e}")
